# 🧠 AI Knowledge Auditor

**Trust and Traceability Tool for LLM Outputs**

## 🚀 Project Overview

AI Knowledge Auditor is a Streamlit-based tool designed to evaluate the **trustworthiness** of answers generated by Large Language Models (LLMs) against reliable documents. By uploading PDFs and prompting the LLM, the tool retrieves relevant context, generates an answer, and checks how well the response aligns with the source material.

## 🎯 Problem Statement

Despite their impressive fluency, LLMs are prone to **hallucinations**—producing confident but factually incorrect answers. This presents a critical challenge in high-stakes domains like healthcare, law, or education.

**The goal of this project is to build a system that:**
- Accepts PDF documents as a source of truth.
- Lets users input questions based on those documents.
- Uses a RAG (Retrieval-Augmented Generation) pipeline to generate answers.
- Compares generated answers with retrieved source chunks using semantic similarity.
- Outputs a **trust score**, highlights mismatches, and flags hallucinated content.

## 🛠️ Tech Stack
- **Frontend**: Streamlit
- **LLM**: Mistral-7B via Hugging Face (or similar)
- **Retrieval**: FAISS vector DB + Sentence Transformers
- **Evaluation**: Cosine similarity, trust score metrics, mismatch highlighting

## 📌 Features
- Upload and chunk multiple PDFs
- Ask questions about the uploaded documents
- Get answers with trust score & visual mismatch feedback
- Deployable as a web app

## 📦 Future Additions
- Source document citation matching
- Paper/pdf evaluator for quality of evidence
- API mode for third-party integration

---
